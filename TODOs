CodeGen for tile size == 1 for all trees
    * [Done] Add threshold and feature index type to tree type
    * [Done] Add tree types to forest type (list of individual tree types)
    * [Done] Write tiling helpers
        - [Done] Serialize model into file
        - [Done] Read model from file into buffer
     
    * [Done] LoadOp for tiled trees stored as memrefs
    * [Done] BUG The length of each tree needs to be passed to the memref subview op
    * [Done] Write pass to replace node values flowing through the IR with index values
        - Remove the NodeToIndex and IndexToNode ops
    * [Done] Lower all other dialects to LLVM
    * [Done] Lower custom tree ops to LLVM dialect
        - Non vector implementation
    * [Done] Initialization routines for length and offset memrefs need to be implemented
    * [Done] Seperate out the tests and write some basic test infrastructure
    * [Done] Write a set of tests that generate small forests and test generated code against that
    * [Done] Write some basic debugging infrastructure for JIT'ed code
    * [Done] Tests with batch size > 1
    * Compare results with XGBoost for small models
        * Write a script to generate test cases that have the following : 
            - JSON file
            - Input and output CSV
        * Write tests in C++ that read these, compile and check results
    * Compare results with XGBoost for full models


CodeGen for tile size != 1
    - Serialization when tile sizes are not 1
    - Vector implementations of tree ops

Code gen for unequal tile size across trees
    - Serialization when tile sizes are unequal across trees
    - Generate >1 model memrefs
    - ??

* Make the look-up in ForestJSONReader based on the name of a global rather than the tile size etc. We're unnecessarily constraining the IR to only have a single ensemble constant with a particular (tile size, threshold type, feature index type) combination now. Instead we should just generate a unique global name for each ensemble constant.

* Generate method to initialize model memref that takes buffers containing actual values as arguments
    - This will be safer than guessing the layout of the tile struct (since we'll pass the thresholds and indices as separate arrays)

* [Done] Memref of memrefs, Memref of struct types
* [Done] Initialization of the model memrefs
    - Method to figure out the storage needed for each tree (number of nodes, size of each node)
    - Generate constants that can be used to initialize the model memref
* Add SIMD and Pipeline Containers
    - How will an Op contain other Ops?!
* Type inference and checking passes (NodeType and TreeType)
* Verify whether the features of the input are indexed with base 1 or 0 in XGBoost
* Tree types need to be part of the ensemble type (we may need to be able to tell tree tiling for a certain tree for example)

* [Defer] MLIR parse routines

* [Done] Add tiling information to tree types
    - Tiling info needs to be in two places -- the DecisionTree class and the TreeType class
* [Done] [Bug] The tensor from which result[i] is being read is not the "iteration tensor" of the the tree loop. 
* [Done] TreeEnsemble attribute
    - Needs to take the TreeEnsemble object and construct a storage class and attribute object
    - Hash for the storage key needs to be figured out (Type, TreeEnsemble)
* [Done] API to build TreeEnsembleAttribute from JSON parser
* [Done] Calls to Builder interface from JSON parser
    - Build methods for the predict op
    - Only needs to build two ops and set the corresponding attributes
* [Done] MLIR print routines for dialect
* [Done] Add batch size to high level IR 
* [Done] Tree constant op
* [Done] Forest constant op
* [Done] Figure out how to implement node types (sub-typing)
* [Done] Add document to describe the next level IR
* [Done] Implement the tree attribute
* [Done] Add TraverseTile for constant tree


ISSUES
1. How do we know which loop is the batch loop(s) and which loop is the tree loop(s) in the mid-level IR? For example, say we want to tile, we've already lost the information about which loop we're dealing with -- we'll have to infer it from what the loop index is being used for!