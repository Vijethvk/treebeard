\section{Optimizations on mid-level IR}
\label{Sec:MIR}
In this section, we present various tree walk optimizations performed by
\Treebeard{} on the mid-level IR.  

\subsection{Tree Walk Interleaving}
A key bottleneck we found when we profiled code generated from tiled walks (even 
after vectorization) was that true dependencies between instructions were still 
causing a significant number of processor stalls. Performing a walk with a 
single input-tree pair did not provide enough independent instructions to keep 
the processor busy. In order to address this, \Treebeard{} applies an 
unroll-and-jam transformation on the innermost loops of the loop nest. This has 
the effect of walking multiple tree and input row pairs in an interleaved 
fashion.  This mitigates the dependency stalls by enabling scheduling of 
instructions from independent tree walks. 

This optimization is performed in two steps. First, a pass on the mid-level IR 
transforms the loop structure.  It unrolls the innermost loops of the loop nest 
a specified number of times and jams together tree walks from the different 
iterations.  The following listing shows the mid-level IR when the inner loop 
over the input rows is unrolled by a factor of two and the two resulting tree 
walks are jammed together.

\begin{lstlisting}[style=c++]
  for t = 0 to numTrees step 1 {
    for i = 0 to batchSize step 2 {
      tree = getTree(forest, t)
      pred1, pred2 = InterleavedWalk((tree, rows[i]),
                                     (tree, rows[i+1]))
    }
  }
\end{lstlisting}
Next when lowering, the operations to traverse each of the tree, input row pairs 
(the arguments to the \texttt{InterleavedWalk}) are interleaved. One step of the interleaved 
walk is listed below. 
\begin{lstlisting}[style=c++]
  // ... 
  tile1 = tile2 = getRoot(tree)
  // ...
  threshold1 = loadThresholds(tree, tile1)
  threshold2 = loadThresholds(tree, tile2)
  featureIndex1 = loadFeatureIndices(tree, tile1)
  featureIndex2 = loadFeatureIndices(tree, tile2)
  feature1 = rows[i][featureIndex1]
  feature2 = rows[i][featureIndex2]
  pred1 = feature1 < threshold1
  pred2 = feature2 < threshold2
  tile1 = getChildTile(tile1, pred1)
  tile2 = getChildTile(tile2, pred2)
  // ...
\end{lstlisting}

\CommentOut{
These transformations are fairly general and are not aware of the in-memory representation of the model. Therefore, they 
are reusable across different in-memory representations - the ones that are currently built into Treebeard or ones that 
maybe added in the future.
\TODO{AP I feel the way this section is currently written makes the optimization seem extremely trivial. Is there a different 
way to present it?}
}
\subsection{Tree Walk Peeling and Tree Walk Unrolling}
\Treebeard{} splits the loop that performs a tree walk into two parts. It peels 
and introduces a \op{prologue} loop that walks down the tree a constant number 
of steps (for example, up to the depth of the first leaf) and then performs the 
rest of the tree walk in a separate loop.

%% As can be made aware (through a simple pass on the IR) of the depth of the first leaf in a tree, 
%First it identifies the leaf with the minimum depth through a simple pass.  Then

% Next \Treebeard{} unrolls the prologue completely, avoiding all traversal induced branching in it.

Several rewrites of the peeled loop are possible. \Treebeard{} completely 
unrolls the prologue if the peeled loop walks the tree upto the depth of the 
first leaf. In cases where \Treebeard{} has already padded and balanced the tree 
(Section~\ref{sec:treeorder}), unrolling the prologue loop completely avoids all 
traversal induced branching.  In the case of probability-based tiling, the 
prologue loop enables specialization of leaf checks so that these checks are 
faster (and less general) for the most probable leaves.

% \TODO{kr: Add example?}

\subsection{Parallelization}
Currently, \Treebeard{} performs a na\"ive parallelization of the inference computation. When parallelism is enabled, the 
loop over the input rows is parallelized using MLIR's OpenMP support. \Treebeard{} rewrites 
the mid-level IR by tiling the loop over the input rows with a tile size equal to the number of cores. 
As a concrete example, consider the case where we intend to perform inference 
using a model with four trees on a batch of 64 rows. Further, assume that we 
wish to parallelize this computation across 8 cores.  \Treebeard{} then 
generates the following IR:
\begin{lstlisting}[style=c++]
  parallel.for i0 = 0 to 64 step 8 {
    for i1 = 0 to 8 step 1 {
      i = i0 + i1
      prediction = 0
      for t = 0 to 4 step 1 {
        tree = getTree(forest, t) 
        treePrediction = WalkDecisionTree(tree, rows[i])
        prediction = prediction + treePrediction
      }
      predictions[i] = prediction
    }
  }
\end{lstlisting}
Currently, \Treebeard{} does not perform other standard parallelization 
optimizations as they are generic and independent of the 
problem domain. We leave a more thorough exploration of parallelizing decision 
trees to future work.
