\section{Introduction}
% Decision trees are among the most widely used ML models. Cite Kaggle and looking glass 
Decision tree ensembles are one of the most popular classes of machine learning models \cite{KaggleSurvey,LookingGlass}.
They are generated by machine learning techniques like gradient boosting and random forests. 
The Kaggle state of data science and machine learning survey \cite{KaggleSurvey} shows that 
these are the most widely used classes of models among data scientists. An analysis of 
several machine learning pipelines that are in production use in a real large scale web company showed that 
gradient boosting machines and random forests were the two most widely used ML algorithms in the company \cite{LookingGlass}.
Not only are decision tree based models widely used, they are also used in a diverse range of 
applications\cite{DecisionTreesOverview}. GBM models were used in the CERN large hadron collider
to classify particles based on the data collected from the collider\cite{LHCModel}. Other applications 
include search engines\cite{YahooSearch}, prediction of the financial performance of companies\cite{Finance},
medical diagnostics\cite{Med1, Med2} and recommendation and notification systems\cite{Facebook}.

% Decision tree ensemble overview
To compute the prediction of a decision tree on an input row $x$\footnote{An input row is a
vector of numbers.}, a path from the root to leaf is computed.
Each node of a decision tree has a threshold value $v$ and a feature index (or column index) $i$.
At a node, if $x_i < v$, then 
the walk moves to the left child. If not, we move to the right child. When a leaf is reached, 
the value of the leaf is returned as the prediction of the tree. 
Most decision tree based techniques combine several trees into a forest (or ensemble) to improve prediction accuracy.
To compute the prediction of the forest, the prediction of each tree is computed and these predictions are 
then combined (usually by adding them). 

% Inference on decision trees is hard 
% Pointer chasing -- cache performance, branch prediction, dependency stalls, not easy to vectorize
Given the wide spread use of decision tree ensembles, the ability to perform high throughput and low latency inference is important.
\TODO{AP Since we don't talk about latency or throughput explicitly, should we be saying this?}
However, optimizing the tree walks required for decision tree inference on modern CPUs is not straight-forward. 
Firstly, naively implemented tree walks have poor spatial and temporal locality and therefore poor cache performance \cite{FAST,MilindTreeVectorization}.
Additionally, since the decision of which node to evaluate next can only be made after evaluating the current node,
true dependencies between instructions cause several pipeline stalls. Also, using SIMD instructions to accelerate 
tree walks is extremely challenging \cite{MilindTreeVectorization}.

Several systems like XGBoost\cite{XGBoost}, LightGBM\cite{LightGBM} and Sklearn\cite{Sklearn} implement
decision tree ensemble inference. These are libraries and implement some optimizations. However, these optimizations are hand
coded and need to be manually rewritten or redesigned for every target that is supported. Also, these 
systems cannot apply specialize inference code depending on the model. A compiler based approach is 
needed to address these problems. However, existing compilers only support fixed code generation techniques\cite{Treelite, Hummingbird}.
To fill the need for an extensible compiler infrastructure for decision forest ensembles, we design and 
build \Treebeard{}\footnote{In J. R. R. Tolkein's The Lord of the Rings, Treebeard was the oldest of the Ents left in Middle-earth, an 
ancient tree-like being who was a ``shepherd of trees''.}, an optimizing compiler for decision tree ensemble inference.
Our motivations for building \Treebeard{} are as follows. 
\begin{enumerate}
  \item Existing libraries \cite{XGBoost, Treelite, LightGBM, VPred} perform specific optimizations and are hard to maintain as hardware evolves. 
  Additionally, they cannot tailor inference code to the specific model being used. Past work has identified that the optimizations required change with the model
  and architectural parameters like cache size\cite{CacheConscious1, CacheConscious2, VPred}. 
  \item The repeated effort to optimize and maintain libraries on several targets is prohibitive. The proliferation of new architectures
  further exacerbates this problem. Compilers have been successful in alleviating this problem in other domains\cite{Halide, TVM}.
  However, no such compiler infrastructure currently exists for decision tree ensembles.
  \item Currently, no system exists that allows a thorough exploration of the optimization space for decision tree ensemble inference. 
  Compilers have been successful in addressing this problem with ML models like DNNs \cite{TVM, Tiramisu, XLA}. However, compiler techniques for
  decision tree ensembles are less well studied.
\end{enumerate}

We make the following contributions.
\begin{enumerate}
  \item We design and build \Treebeard{}, an extensible compiler infrastructure for decision tree model inference. The 
  infrastructure is built to allow exploration of optimization and code generation techniques. \TODO{Is this claim too grand?}
  \item We develop a general infrastructure for the vectorization of decision tree walks based on grouping tree nodes into ``tiles''.
   This includes general support for code generation and the in-memory representation of tiled trees. The infrastructure can be 
   used to tile trees based on different cost functions.
  \item We show that trees can be tiled using different cost functions. We present two novel tiling methods that are implemented
  using the general tiling infrastructure. 
  \item We design and implement various model and loop transformations that significantly improve generated inference code performance 
  to show the power of the proposed framework.
\end{enumerate}


