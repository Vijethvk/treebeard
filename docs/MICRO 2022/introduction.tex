\section{Introduction}
\label{sec:intro}
Decision tree ensembles are among the most popular classes of machine learning (ML) models~\cite{KaggleSurvey,LookingGlass}. The Kaggle state of ML survey 2021 \cite{KaggleSurvey} shows that 
more than 70\% of data scientists use decision tree ensembles while less than 40\% use neural networks. 
Such ensembles are generated by ML techniques like gradient boosting and random forests, reported to be the top
two algorithms used in production pipelines at a large scale web company\cite{LookingGlass}. Such models also 
have many diverse use cases\cite{DecisionTreesOverview}. Not only are they used in computer science applications 
like search~\cite{YahooSearch}, recommendation and notification systems\cite{Facebook}, but also in 
financial~\cite{Finance} and medical~\cite{Med1, Med2} applications. Gradient boosted models (GBM) are
even used in the CERN large hadron collider to classify particles\cite{LHCModel}. 

\CommentOut{
Decision tree ensembles simply aggregate predictions made by a set of decision trees.
To compute the prediction of a decision tree on an input row $x$\footnote{An input row is a
vector of numbers.}, a path from the root to leaf is traversed.
Each node of a decision tree has a threshold value $v$ and a feature index $i$.
At a node, if $x_i < v$, then
the walk moves to the left child. If not, it moves to the right child. When a leaf is reached,
the value of the leaf is returned as the prediction of the tree.

%Most decision tree-based techniques combine several trees into a forest (or ensemble) to improve prediction accuracy.
%The prediction of each tree is computed and these predictions are 
%then combined (usually by adding them). 
}

A recent survey~\cite{SageMaker} that breaks down the cost of ownership of a
data science solution among different stages indicates that inference costs
have now become the dominant (in the range 45\%-65\%) component~\cite{Hummingbird}. This motivates
a careful study of the efficiency of inference with decision tree ensembles.
Such inference involves a simultaneous traversal of multiple decision trees.
Given an input row, a path from the root to leaf is traversed for each tree and
the values at the leafs are aggregated together to produce a prediction.  At
each internal node an input feature $x_i$ is compared with a threshold value $v$ for that node
to determine whether the walk moves left or right\footnote{  
The condition $x_i<v$ is the \emph{node predicate} or the \emph{node condition}.
At each node, if the node predicate is true, then
the walk moves to the left child. If not, it moves to the right child. When a leaf is reached,
the value $v$ of the leaf is returned as the prediction of the tree.}.
 
Optimizing tree walks on a CPU is challenging because they have irregular access
patterns and are sensitive to various architectural parameters.  A profile of a
basic traversal shows that a na\"ively implemented tree walk has poor spatial
and temporal locality, and therefore poor cache performance~\cite{FAST,MilindTreeVectorization}.  Frequent branching and true dependencies
between instructions cause several pipeline stalls. Furthermore, accelerating tree
walks with low level optimizations like vectorization using SIMD instructions is
extremely challenging~\cite{MilindTreeVectorization}.

Today, the most popular systems for performing inference for tree-based models
are libraries like XGBoost\cite{XGBoost}, LightGBM\cite{LightGBM} and
Sklearn\cite{Sklearn}.  As one would expect, these libraries implement a fixed
set of optimizations for the hardware configurations they support. As hardware
evolves, specializing the library to newer generations gets prohibitively
expensive and usually has a high lead time. Further, these libraries do not
specialize inference to the specifics of the model being used for inference.

Recent research has seen the advent of optimizing compilers for deep neural
networks~\cite{Halide, TVM, TensorComprehensions, Tiramisu, XLA}. Such compilers
have been shown to perform much better than domain-specific libraries in several
cases.  Surprisingly, despite their widespread usage, very few compilers exist
for decision tree ensembles~\cite{Treelite, Hummingbird}.  One 
system, Hummingbird~\cite{Hummingbird} builds on the success of DNN compilers by
transforming decision tree traversals to tensor operations. However, it sometimes
introduces more expensive operations like matrix multiplication
just to be able to leverage compilers
like TVM~\cite{TVM} for efficient code generation. As their results indicate,
when it comes to tree-based models this strategy does not always succeed: some
benchmarks' performance degrades when compared to a state-of-the-art library. 
Treelite\cite{Treelite}, the only other compiler for tree inference, uses 
a simple compilation strategy. It aggressively expands all trees in the 
model into \op{if-else} statements. As we show later in the paper, this strategy
is quite limited in applying various optimizations.

This paper presents the design and implementation of \Treebeard{}\footnote{In 
J. R. R. Tolkein's The Lord of the Rings, Treebeard was the oldest of the Ents left in Middle-earth, an 
ancient tree-like being who was a ``shepherd of trees''.}, an extensible optimizing compiler infrastructure for decision tree ensemble
inference. \Treebeard{} enables three different classes of optimizations: (i) pertaining to the nature of the algorithm, i.e., simultaneous traversal of several trees.
(ii) pertaining to the properties of the tree being traversed, like imbalances in structure and probabilities of reaching leaves. 
(iii) targeting the characteristics of the CPU, like support for vector instructions. 
We carefully construct the compiler so that different optimizations are applied at different intermediate
representations of the computation. By doing so, \Treebeard{} facilitates the selection and composition of
several optimizations at each level of abstraction, as well as automatic and efficient code generation.

We implement \Treebeard{} using the MLIR compiler infrastructure\cite{MLIR} and evaluate it 
on a diverse set of decision tree-based models. We report that the code generated by
\Treebeard{} is significantly faster than state-of-the-art systems: it is on average
$2.6\times$ and $4.7\times$ faster than XGBoost and Treelite respectively on a single core. 
Even with a very simple parallelization strategy, \Treebeard{} achieves speedups of $2.3\times$ and $2.7\times$
compared to XGBoost and Treelite on a 16-core system.
% \TODO{kr: is the below line needed}.
% Our optimizations improve performance significantly over a simple \Treebeard{} generated baseline (with comparable single-core performance as XGBoost but better than treelite) 
% on both Intel (by $2.2\times$) and AMD (by $1.8\times$) processors.

In summary, we make the following contributions.
\begin{enumerate}
\item We design and implement \Treebeard{}, an extensible compiler infrastructure for decision tree model inference. 
\Treebeard{} is built to allow exploration of several optimizations and code generation techniques. % \TODO{Is this claim too grand?}
\item We propose novel tiling transformations that reduce the cost of tree walks by grouping tree nodes into ``tiles''.
We propose tiling algorithms that can utilize specific properties of the model being compiled.
\item We design and implement various model and loop transformations that significantly improve generated inference
code performance. \Treebeard{} also enables parallelization of the inference computation.
\item We develop a general infrastructure for the vectorization of decision tree walks.
This includes general support for code generation and memory-layout optimizations for tiled trees.
\end{enumerate}
