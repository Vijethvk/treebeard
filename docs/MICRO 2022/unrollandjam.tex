\subsection{Tree Walk Interleaving}
While tiling and subsequent vectorization gives significant performance gains, profiling 
the generated code showed that true dependencies between instructions were still causing
a significant number of stall cycles. In order to fill these stall cycles, Treebeard does 
an unroll and jam on the inner most loops of the loop nest. This transformation has 
the effect of walking multiple tree and input row pairs in an interleaved fashion. 
The dependency stalls can be hidden by scheduling instructions from independent tree walks 
in the stall cycles. 

This optimization is performed across both the mid-level IR and low-level IR. Loops on which 
to perform unroll and jam are identified in the mid-level IR. This information is communicated to the 
lowering passes by annotating these tree walk operations as descibred in section \ref{sec:Overview}. The lowering 
passes that transform the mid-level IR to low-level IR interleave operations across independent
tree walks.

The following listing shows the mid-level IR when the inner loop over the input rows is unrolled 
by a factor of 2 and the two resulting tree walks are jammed together.

\begin{lstlisting}[style=c++]
  forest = ensemble(...)
  for t = 0 to numTrees step 1 {
    for i = 0 to batchSize step 2 {
      tree = getTree(forest, t)
      prediction1, prediction2 = InterleavedWalk((tree, rows[i]), (tree, rows[i+1]))
    }
  }
\end{lstlisting}

When lowered to low-level IR, the operations to traverse each of the tree, input row pairs 
(the arguments to the \texttt{InterleavedWalk}) are interleaved. One step of the interleaved 
walk is listed below. 
\begin{lstlisting}[style=c++]
  // ... 
  n1 = n2 = getRoot(tree)
  // ...
  threshold1 = loadThresholds(n1)
  threshold2 = loadThresholds(n2)
  featureIndex1 = loadFeatureIndices(n1)
  featureIndex2 = loadFeatureIndices(n2)
  feature1 = rows[i][featureIndex1]
  feature2 = rows[i][featureIndex2]
  pred1 = feature1 < threshold1
  pred2 = feature2 < threshold2
  n1 = getChildNode(n1, pred1)
  n2 = getChildNode(n2, pred2)
  // ...
\end{lstlisting}
These transformations are fairly general and are not aware of the in memory representation of the model. Therefore, they 
are reusable across different in memory representations - the ones that are currently built into Treebeard or ones that 
maybe added in the future.
\TODO{AP I feel the way this section is currently written makes the optimization seem extremely trivial. Is there a different 
way to present it?}