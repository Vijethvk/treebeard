\subsection{Notation}
%TODO{Notation needs to be introduced in the background section, Tiling should be the first subsection}
We represent a decision tree by $\Tree = (V, E, r)$ where $V$ is the set of nodes, $E$ the set of edges and
$r \in V$ is the root. For each node $n \in V$, we define the following.
\begin{enumerate}
    \item $threshold(n) \in \mathbb{R}$, the threshold value for $n$.
    \item $featureIndex(n) \in \mathbb{N}$, the feature index for $n$.
    \item $left(n) \in V$, the left child of $n$ or $\emptyset$ if $n$ is a leaf. If $left(n) \neq \emptyset$, then $(n, left(n)) \in E$.
    \item $right(n) \in V$, the right child of $n$ or $\emptyset$ if $n$ is a leaf. If $right(n) \neq \emptyset$, then $(n, right(n)) \in E$.
\end{enumerate}
We use $L_{\Tree} \subseteq V$ to denote the set of leaves. % subseteq because tree could have single node

\subsection{Tree Tiling}
\label{sec:Tiling}
% Treebeard vectorizes tree walks by grouping nodes of a decision tree into \textbf{\emph{tiles}}. The nodes in a tile are evaluated concurrently using vector instructions. Once the nodes of the current tile are evaluated, a look up table is used to compute which child of the current tile to move to next.

While a decision tree is naturally represented as a binary tree, 
this is not an efficient representation for tree traversal as it (i) requires many memory accesses, 
(ii) has poor branching structure which may cause high branch misprediction rates and (iii) cannot use vector instructions. 
This section proposes a tiling optimization where multiple nodes of a decision tree are grouped into a single tile,
effectively transforming a binary tree into an $n$-ary tiled tree. 
Note that an ideal decision tree traversal visits only a subset of nodes in a tile, with strict dependency between nodes at different levels.
Unfortunately, this dependency prevents parallelization/vectorization of the tree walk. However, by speculatively evaluating conditions 
of all nodes in a tile, we can eliminate this dependency and enable
the compiler to generate vectorized code (see Section~\ref{sec:Vectorization}) to traverse trees.
Secondly, this enables spatial locality improvements by grouping together nodes that are likely to be accessed together (Section~\ref{Sec:MemoryRep}).   
We present two different tiling heuristics later in this section. 

\CommentOut{
Treebeard groups nodes of the decision tree into \textbf{\emph{tiles}}. Tiling provides two benefits. 
\begin{enumerate}
  \item It allows the compiler to generate vector code to traverse trees. Section \ref{sec:Vectorization} describes how Treebeard does this.
  \item It enables spatial locality improvements by grouping together nodes that are likely to be accessed together. 
\end{enumerate}
Once nodes are grouped into tiles, an $n$-ary tree whose nodes are tiles is constructed. Treebeard then generates 
optimized code to walk this tree. The listing below shows at a high level how a tiled tree is walked (This is not 
true IR, but presented for clarity). 
}

Once trees are tiled \Treebeard{} generates tree walks with the code structure shown below.
\begin{lstlisting}[style=c++]
  WalkDecisionTree(...) {
    tile = getRootTile(tree)
    while (!isLeaf(tree, tile)) do {
      // Evaluate predicates of all nodes in the tile
      predicates = evaluateTilePredicates(tile, rows[i])
      
      // Move to the correct child of the current tile
      tile = moveToChildTile(tree, tile, predicates) 
    }
    treePrediction = getLeafValue(tile)
  }  
\end{lstlisting}
The code is an abstract representation of a tiled tree walk that enables efficient lowering of specific steps in subsequent stages. 
\op{evaluateTilePredicates} (speculatively) computes the predicates of all nodes in a tile (line 6)\footnote{ 
\op{evaluateTilePredicates} expands to the operations from line 10 to 15 in the listing in Section~\ref{sec:Vectorization}.}.
Then \op{moveToChildTile} (line 9), 
uses the computed predicate values to determine which child of the current tile to move to\footnote{\op{moveToChildTile} expands 
to the operations from line 18 to 25 in the listing in Section~\ref{sec:Vectorization}.}. We defer a description of how these 
operators are lowered to Section~\ref{sec:Vectorization} and instead focus on tiling algorithms in this section.


\CommentOut{
To compute the prediction of the tree, the predicates of all nodes in the tile are computed simultaneously (line 6). 
Then, the computed predicate values are used to determine which child of the current tile to move to (in the 
$n$-ary tree). This section presents the details of tiling and how Treebeard's general tiling infrastructure 
can be used to develop tiling algorithms with different objectives (sections \ref{sec:UnifTiling}) and \ref{sec:ProbTiling}).
The details of how Treebeard lowers predicate evaluation and moving to the correct child to use vector instructions 
are described in section \ref{sec:Vectorization}.

\subsubsection{Tiles and Tree Tiling}
\label{sec:ValidTiling}
}
\subsubsection{Conditions for Valid Tiling}
\label{sec:ValidTiling}
While any arbitrary partitioning of the nodes of a tree could be considered for tiling, we impose a 
few simple constraints to simplify the design of the compiler.
% to only allow nodes that are likely to be accessed together to be grouped into a tile. 
% \TODO{kr : replace $n_t$ with sz?}
Given a tree $\Tree = (V, E, r)$ and a tile size $n_t$ we impose the following constraints on the generated tiles $\{ T_1, T_2, ... ,T_m \}$.
\begin{description}
    \item[Partitioning:] $T_1 \cup T_2 ... \cup T_m = V$ and $T_i \cap T_j = 
    \emptyset$ for all $i\neq j$.
    \item[Connectedness:] If $u, v \in T_i$, there is a (undirected) path connecting $u$ and $v$ fully contained in $T_i$.
   \item [Leaf separation:] $\forall l \in L_{\Tree}$ : $l \in T_i \rightarrow v \notin T_i \;\; \forall v \in V \backslash \{l\}$.
  \item [Maximal tiling:] if there are tiles such that $|T_i| < n_t$, then there 
is no $v \in V\backslash \{ T_i \cup L_{\Tree} \}$ such that $(u, v) \in E$ for 
some $u \in T_i$.  \end{description}
The \textbf{partitioning} and \textbf{maximal tiling} constraints together ensure that we group nodes into as few tiles as possible. {\textbf{Connectedness}} ensures that each tile is a sub-tree, a natural grouping of nodes that are likely to be accessed together. The {\textbf{leaf separation}} constraint ensures that leaves are not grouped with internal nodes, as leaves in a decision tree need special handling and are used to check for walk termination and to determine the output (prediction). Thus, the {\textbf{leaf separation}} constraint ensures that tiles are homogeneous. This in-turn allows \Treebeard{}  to specialize the in-memory layout of trees and also simplifies code generation. We discuss leaf handling and tree layout in Section ~\ref{Sec:MemoryRep}. We refer to any tiling that satisfies the above constraints as a \emph{valid} tiling.

%%% COMMENT %%%% 
\CommentOut{
\subsection{Tiled Trees}
A tiling transformation communicates the tiling to the Treebeard infrastructure by assigning a tile ID to each node in the decision tree. Using these tile IDs, Treebeard checks the validity of the tiling and then contructs a tree whose nodes are tiles. We call this tree the \textbf{\emph{tree of tiles}}. \TODO{We need a better name for this}
Figure \ref{Fig:ValidTilingTileSize3} shows a valid tiling with tile size 3 and the tree of tiles constructed by Treebeard. Three nodes are grouped into each of the tiles $t_1$ and $t_2$ as shown. Each tile is collapsed into a single node in the tree of tiles. However, each leaf in the original tree becomes a leaf in the tree of tiles.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/TiledTree_Size3.PNG}
  \caption{Example of a valid tree tiling with tile size $n_t=3$}
  \label{Fig:ValidTilingTileSize3}
\end{figure}

Treebeard maintains the following invariants.
\begin{enumerate}
  \item All tiles in a tree are the same size $n_t$. If the tiling produces any smaller tiles, these are padded by inserting dummy nodes to make them the required size.
  \item Nodes within tiles are always ordered in level order and left to right within a level. The numbering of the nodes in the above diagram shows this node order.
  \item Children of a node are numbered from left to right (regardless of level). For example, $l_1$ is the first child of $t_1$, $l_2$ is the second and so on.
\end{enumerate}
}
%%% END COMMENT %%%%

\subsubsection{Rationale for Different Tiling Methods}
\label{sec:rationale}

Before we present different tiling methods, we first discuss an interesting property of decision trees and 
explore whether it can be exploited to optimize tree traversal. We find that the probabilities of reaching different leaves
in a tree can vary significantly, making some root to leaf paths more frequently traversed than others. As the set of 
decision trees that are walked for a given ML model is fixed, we ask whether this statistical property can be 
exploited to generate efficient traversal code.% We address this question by developing different decision tree tiling methods. 

To understand how often different leaves are reached, let us look at the percentage of leaves required to 
cover a significant part (say 80\% or 90\%) of the training data\footnote{We use the training data to compute tree statistics 
since the test data must have a distribution similar to the training data.}.
% in the decision tree ensemble. 
This is captured in Figures~\ref{Fig:AirlineOHEStats} and~\ref{Fig:EpsilonStats}.
%% that are generated from training data. 
Each line in these graphs corresponds to a fixed fraction  (say $f$) of the training data. 
A point on this line at coordinate $(x, y)$ means that a fraction $y$ of trees in the model could cover a
fraction $f$ of all training inputs with a fraction $x$ of leaves. For example, the first point on the
$f=0.9$ line in Figure \ref{Fig:AirlineOHEStats}(at the bottom left of the plot) indicates that about 52\% of trees ($y$ value) need only 1\% of their
leaves ($x$ value) to cover 90\% of the training input. 
In general, Figure~\ref{Fig:AirlineOHEStats} shows that very few leaves are needed to cover a very large fraction 
of inputs for the benchmark \op{airline-ohe}. This means that a small fraction of leaves are very likely. 
We call trees with a small number of extremely likely leaves \textbf{\emph{leaf-biased}}.
On the other hand, for the benchmark \op{epsilon},
Figure \ref{Fig:EpsilonStats} shows that trees need a much larger fraction of their leaves to cover a significant fraction of the training input.
This means that most trees in \op{epsilon} are not leaf-biased. For leaf-biased trees, it would be beneficial to tile nodes such that the
depth of the most probable leaves are minimized even at the expense of 
increasing the depth of the less probable leaves. We call the tiling algorithm 
designed to do this \emph{probability-based tiling} (Section \ref{sec:ProbTiling}). 

% \TODO{AP One hole here is why doesn't this method of tiling work for unbiased trees.}

\begin{figure*}[tbp]
  \centering
  \subfloat[\texttt{airline-ohe}]{
    \centering
    \includegraphics[width=0.35\linewidth]{figures/airline-ohe.png}
    \label{Fig:AirlineOHEStats}
  }
  %\newline
  \hspace{2cm}
  \subfloat[\texttt{epsilon}]{
    \centering
    \includegraphics[width=0.35\linewidth]{figures/epsilon.png}
    \label{Fig:EpsilonStats}
  }
  \caption{Statistical profiles for \texttt{airline-ohe} and \texttt{epsilon}.}
\end{figure*}

% From the above discussion, we can infer that when the decision tree ensemble is not leaf-biased, it would be beneficial to tile the trees in 
% such a way that the depth of the tree (in terms of the number of tiles) is minimized across all the leaves (assuming there is no major skew 
% in the probabilities of different leaves).  

However, when there is no clear leaf-bias, a reasonable objective is to minimize the number of nodes executed 
speculatively. An intuitive heuristic to achieve this objective is to minimize the depth of each tile 
that is constructed. \emph{Basic tiling} (Section \ref{sec:UnifTiling}) is a heuristic designed to do this. 
It is possible to define other variants of tiling(e.g., minimize the maximum leaf depth), but we leave this for future work.
We believe that \Treebeard{}'s tiling infrastructure can support and optimize all such variants that produce valid tilings.

\CommentOut{
\subsubsection{The Optimization Problem}

%We assume that we are given the probabilities of each leaf node of the decision tree (these can easily be computed using the training data). For every leaf $l \in L$, we %are given the probability $p_l$ that the leaf $l$ is reached. 

Observe that the latency of one tree walk is proportional to the number of tiles that need to be evaluated to reach the leaf. It is easy to see that for a leaf biased tree, basic tilling does not optimize for this objective, it considers all leaves to be equally likely. 
 
The goal of probablistic tiling is to minimize the average inference latency, or equivalently the minimize the expected number of tiles that are evaluated to compute one tree prediction. More formally, the problem is to find a \emph{valid} (as defined in Section~\ref{sec:ValidTiling}) tiling $\mathcal{T}$ such that the following objective is minimized.
\[
    \min_{\mathcal{T} \in \mathcal{C}(T)}{\sum_{l \in L_{\Tree}} p_l.depth_{\mathcal{T}}(l)}
\]
where the minimization is over all valid tilings $\mathcal{T}$ of the tree $\Tree$, $depth_{\mathcal{T}}(l)$ is the depth of the leaf $l$ given tiling ${\mathcal{T}}$. $p_l$ is the probability of of reaching leaf $l$ as observed during training.

The above optimization problem can be solved optimally using dynamic programming. 
We leave this out in the interest of space. 
Instead, we use the simple greedy algorithm listed in algorithm \ref{Alg:GreedyTilingAlgo} to construct a valid tiling given the node probabilities\footnote{Probabilites for internal nodes can be computed from probablities for leaves by summing up the probabilities of all leaves that belong to the sub-tree rooted at the internal node. Leaf probabilities are collected during training.}.
The algorithm starts at the root and greedily keeps adding the most probable legal node to the current tile until the maximum tile size is reached.
Subsequently, the tiling procedure is recursively performed on all nodes that are destinations for edges going out of the constructed tile.
}



\input{probtiling}
\input{uniftiling}


\subsection{A Note on Implementation}
%\TODO{Kr : Not sure if this is needed}
The tiling algorithms generate a \op{TileId} attribute per tree. The \op{TileId} attribute 
contains a mapping from a Node to the TileId assigned to it.
This information is used when lowering to the mid level abstraction in the form of loops. 
%A sample of the lowered MIR code is shown in Figure~\ref{Fig:Overview}.	

