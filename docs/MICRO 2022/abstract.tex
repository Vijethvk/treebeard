\begin{abstract}
  Decision tree ensembles, are a commonly used machine learning model. Such models are
  generated when using machine learning techniques like gradient boosting and random 
  forests and are used in a wide range of applications from recommendation systems 
  to tabular data analysis. They are also deployed at scale in the enterprise.
  
  Several libraries such as XGBoost, LightGBM and Sklearn expose algorithms for both
  training and inference with decision tree ensembles. These systems incorporate a 
  limited set of optimizations usually targeted at specific hardware. Further, existing
  systems do not specialize the inference code to the model being used.
  
  This paper presents Treebeard, an extensible compiler for decision tree models.
  Treebeard progressively lowers inference code to LLVM IR through multiple intermediate
  representations. By applying model specific optimizations at the higher levels, loop
  optimizations at the middle level and machine specific optimizations lower down,
  Treebeard can specialize inference code for each model on each supported
  hardware target. Treebeard performs several novel optimizations such as tree tiling,
  tree walk unrolling and tree walk interleaving to improve performance of decision
  tree ensemble inference.
  
  We implement Treebeard using the MLIR compiler infrastructure and
  demonstrate the utility of Treebeard by evaluating it on a diverse set of
  tree ensemble models. We report that Treebeard optimizations improve
  average latency over a batch of inputs by 2.2X in comparison to an unoptimized baseline.
  Further, we find that Treebeard is significantly faster than other frameworks like
  XGBoost (3X on average) and Treelite (5X on average) in both single and multi-core
  settings.
\end{abstract}
