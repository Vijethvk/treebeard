\begin{abstract}
  Decision tree ensembles are among the most commonly used machine learning
  models. These models are used in a wide range of applications and are deployed
  at scale. Inference with decision tree ensembles is usually performed with
  libraries such as XGBoost, LightGBM, and Sklearn. These libraries incorporate
  a fixed set of optimizations for the hardware targets they support. However,
  maintaining these optimizations is prohibitively expensive with the evolution
  of hardware.  Further, they do not specialize the inference
  code to the model being used, leaving significant performance on the table.

  This paper presents \Treebeard{}, an optimizing compiler that progressively
  lowers  inference code to LLVM IR through multiple intermediate abstractions.
  By applying model-specific optimizations at the higher levels, tree walk
  optimizations at the middle level, and machine-specific optimizations lower
  down, \Treebeard{} can specialize inference code for each model on each
  supported hardware target. \Treebeard{} combines several novel optimizations
  at various abstraction levels to mitigate architectural bottlenecks and enable
  SIMD vectorization of tree walks.

  We implement \Treebeard{} using the MLIR compiler infrastructure and
  demonstrate its utility by evaluating it on a diverse set of benchmarks.
  \Treebeard{} is significantly faster than state-of-the-art systems, XGBoost
  and Treelite: by 2.6$\times$ and 4.7$\times$ respectively in a single-core
  execution setting, and by 2.3$\times$ and 2.7$\times$ respectively in
  multi-core settings.

\end{abstract}


  %generated by techniques like gradient boosting and random forests.

  % (run repeatedly on billions of data items).
 % such as tree tiling, tree walk unrolling, and tree walk interleaving.
 % Experimental evaluation demonstrates that \Treebeard{} optimizations
  % improve average latency over a batch of inputs by 2.2$\times$ over our baseline version.
 %% other frameworks like
