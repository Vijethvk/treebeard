\begin{abstract}
  Decision tree ensembles are commonly used machine learning models
  generated by machine learning techniques like gradient boosting and random forests. 
  These models are used in a wide range of applications and are deployed at scale
  (run repeatedly on billions of data items). Several libraries such as XGBoost, 
  LightGBM, and Sklearn expose algorithms for both training and inference with 
  decision tree ensembles. While these libraries 
  incorporate a limited set of hardware-specific optimizations,
  they do not specialize the inference code to the model being used, 
  leaving significant performance on the table.

  This paper presents a compiler-based approach that automatically generates  
  efficient code for decision tree inference. 
  It develops Treebeard, an extensible compiler, which progressively lowers  
  inference code to LLVM IR through multiple intermediate abstractions.
  By applying model-specific optimizations at the higher levels, loop
  optimizations at the middle level, and machine-specific optimizations lower down,
  Treebeard can specialize inference code for each model on each supported
  hardware target. To improve model inference performance, Treebeard performs several 
  novel optimizations such as tree tiling, tree walk unrolling, and tree walk interleaving.
 
  We implement Treebeard using the MLIR compiler infrastructure and
  demonstrate the utility of Treebeard by evaluating it on a diverse set of
  tree ensemble models. Experimental evaluation demonstrates that Treebeard optimizations 
  improve average latency over a batch of inputs by 2.2X over our baseline version. 
  Further, Treebeard is significantly faster than %% other frameworks like
  XGBoost and Treelite in both single-core (2.8X and 5.1X respectively) and multi-core
  (3.2X and 2.6X respectively) settings.
\end{abstract}
