\begin{abstract}
Forests of decision trees, called decision tree ensembles, are a very commonly used
machine learning model. Machine learning techniques like gradient boosting 
and random forests build forests of decision trees. These models are used in several
applications ranging from recommendation systems to tabular data analysis. They 
are also deployed at scale in the enterprise.

Several systems such as XGBoost, LightGBM and Sklearn implement algorithms based on
decision tree ensembles. However, limited work exists on optimizing the inference 
performance of decision tree based models. Existing systems perform specific 
optimizations but these are hard to maintain as hardware evolves. Additionally, these systems
cannot tailor the inference code to the model being used. Also, no system exists today
that can perform a systematic search of the optimization space. To address all these
problems, we build Treebeard, an extensible compiler infrastructure to compile
decision tree models. The infrastructure is built to allow exploration of optimization 
and code generation techniques. We demonstrate the power of the infrastructure by 
implementing several novel optimizations for decision tree ensemble inference.

We evaluate the performance of code generated by Treebeard and find that our
optimizations improve performance by 2.2X over an unoptimized baseline. 
We compare the performance of 
Treebeard with that of XGBoost and Treelite and find that Treebeard improves
performance over these systems by almost 3X and 5X respectively. \TODO{How do we 
list speedups over XGBoost and Treelite? They vary across batch sizes. Should we 
also state serial and parallel speedups separately?}  


\end{abstract}
