\begin{abstract}
  Decision tree ensembles, are a commonly used machine learning model. Such models are
  generated when using machine learning techniques like gradient boosting and random 
  forests and are used in a wide range of applications from recommendation systems 
  to tabular data analysis. They are also deployed at scale in the enterprise.
  
  Several libraries such as XGBoost, LightGBM and Sklearn expose algorithms for both
  training and inference with decision tree ensembles. These systems incorporate a 
  limited set of optimizations usually targeted at specific hardware. Further, existing
  systems do not specialize the inference code to the model being used.
  
  This paper presents Treebeard, an extensible compiler for decision tree models.
  TreeBeard progressively lowers inference code to LLVM through multiple intermediate 
  representations. By applying model specific optimizations at the higher levels, loop
  optimizations at the middle level and machine specific optimizations lower down,
  TreeBeard is able to specialize inference code for each model on each supported
  hardware target. Treebeard performs several novel optimizations such as tree tiling,
  tree walk unrolling and tree walk interleaving to improve performance of decision
  tree ensemble inference.
  
  We demonstrate the utility of Treebeard by evaluating it on a diverse set of
  tree ensemble models and comparing it against an unpotimized baseline. We report
  that Treebeard optimizations improve average latency over a batch of inputs by 2.2X.
  Further, we find that TreeBeard is significantly faster than other franeworks like
  XGBoost  (3X on average) and Treelite (5X on average) on both single and multi-core
  settings. 
\end{abstract}
